"""
Script de Evaluaci√≥n con LangSmith
Eval√∫a art√≠culos editoriales usando datasets y evaluadores personalizados

Requiere:
    pip install langsmith langchain-openai python-dotenv

Configuraci√≥n:
    export LANGSMITH_API_KEY="tu-api-key"
    export OPENAI_API_KEY="tu-openai-key"

Uso:
    python langsmith_evaluador.py <archivo_markdown>
"""

import os
import sys
import re
from datetime import datetime
from pathlib import Path
from typing import Optional
from dotenv import load_dotenv

from langsmith import Client
from langsmith.schemas import Example
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# Cargar variables de entorno
load_dotenv()

# ============================================================================
# CONFIGURACI√ìN
# ============================================================================

DATASET_NAME = "Art√≠culos Editoriales IA"
EXPERIMENT_PREFIX = "Editorial IA Evaluation"

# Inicializar cliente de LangSmith
client = Client()

# Inicializar LLM para evaluaciones (SIN wrappers)
llm = ChatOpenAI(
    model="gpt-4-turbo",
    temperature=0,
    api_key=os.getenv("OPENAI_API_KEY")
)

# ============================================================================
# EXTRACCI√ìN DEL ART√çCULO
# ============================================================================

def extraer_articulo_del_markdown(archivo_path: str) -> Optional[str]:
    """Extrae el contenido del art√≠culo del archivo Markdown generado"""
    
    try:
        with open(archivo_path, 'r', encoding='utf-8') as f:
            contenido = f.read()
        
        # Buscar secci√≥n "## üìÑ ART√çCULO FINAL"
        patron = r"## üìÑ ART√çCULO FINAL\n\n(.*?)\n\n---"
        match = re.search(patron, contenido, re.DOTALL)
        
        if match:
            articulo = match.group(1).strip()
            return articulo
        else:
            partes = contenido.split("## üìÑ ART√çCULO FINAL")
            if len(partes) > 1:
                articulo = partes[1].split("\n\n---")[0].strip()
                return articulo
        
        return None
    
    except FileNotFoundError:
        print(f"‚ùå Archivo no encontrado: {archivo_path}")
        return None
    except Exception as e:
        print(f"‚ùå Error al procesar archivo: {e}")
        return None


# ============================================================================
# EVALUADORES PERSONALIZADOS
# ============================================================================

def evaluador_longitud_minima(ejemplo: Example) -> dict:
    """
    Evaluador 1: Verifica que el art√≠culo tenga longitud m√≠nima
    M√©trica: ¬øEl art√≠culo tiene al menos 500 caracteres?
    """
    
    try:
        articulo = ejemplo.inputs.get("articulo", "")
        caracteres = len(articulo)
        
        score = 1 if caracteres >= 500 else 0
        
        return {
            "key": "longitud_minima",
            "score": score,
            "comment": f"Art√≠culo tiene {caracteres} caracteres (m√≠nimo: 500)"
        }
    except Exception as e:
        return {"key": "longitud_minima", "score": 0, "comment": str(e)}


def evaluador_contenido_apropiado(ejemplo: Example) -> dict:
    """
    Evaluador 2: Verifica si el contenido es apropiado y sano
    M√©trica: ¬øEl art√≠culo promueve buenas costumbres sin ser ofensivo?
    """
    
    try:
        articulo = ejemplo.inputs.get("articulo", "").lower()
        
        # Palabras/frases inapropiadas a buscar
        palabras_prohibidas = [
            "discrimin", "racist", "sexist", "odio", "violencia extrema"
        ]
        
        tiene_contenido_inapropiado = any(
            palabra in articulo for palabra in palabras_prohibidas
        )
        
        score = 0 if tiene_contenido_inapropiado else 1
        
        return {
            "key": "contenido_apropiado",
            "score": score,
            "comment": (
                "Contenido inapropiado detectado" if tiene_contenido_inapropiado
                else "Contenido apropiado y sano"
            )
        }
    except Exception as e:
        return {"key": "contenido_apropiado", "score": 1, "comment": str(e)}


def evaluador_cantidad_parrafos(ejemplo: Example) -> dict:
    """
    Evaluador 3: Verifica estructura con m√∫ltiples p√°rrafos
    M√©trica: ¬øTiene al menos 4 p√°rrafos bien estructurados?
    """
    
    try:
        articulo = ejemplo.inputs.get("articulo", "")
        parrafos = [p.strip() for p in articulo.split('\n\n') if p.strip()]
        num_parrafos = len(parrafos)
        
        # Buscamos al menos 4 p√°rrafos
        score = 1 if num_parrafos >= 4 else (num_parrafos / 4)
        
        return {
            "key": "estructura_parrafos",
            "score": score,
            "comment": f"Art√≠culo tiene {num_parrafos} p√°rrafos (esperado: ‚â•4)"
        }
    except Exception as e:
        return {"key": "estructura_parrafos", "score": 0, "comment": str(e)}


def evaluador_relevancia_tema(ejemplo: Example) -> dict:
    """
    Evaluador 4: Verifica que el contenido sea relevante al tema
    M√©trica: ¬øMenciona IA, tecnolog√≠a, o pa√≠ses desarrollados?
    """
    
    try:
        articulo = ejemplo.inputs.get("articulo", "").lower()
        
        # Palabras clave relacionadas al tema
        palabras_clave = [
            "inteligencia artificial", "ia", "llm", "gpt",
            "pais", "mundo", "desarrollo", "tech", "innovation",
            "primer mundo", "desarrollado"
        ]
        
        menciones = sum(1 for palabra in palabras_clave if palabra in articulo)
        score = min(1.0, menciones / 3)  # M√°ximo si menciona 3+ t√©rminos clave
        
        return {
            "key": "relevancia_tema",
            "score": score,
            "comment": f"Se encontraron {menciones} t√©rminos clave del tema (esperado: ‚â•3)"
        }
    except Exception as e:
        return {"key": "relevancia_tema", "score": 0, "comment": str(e)}


def evaluador_tono_comico_llm(ejemplo: Example) -> dict:
    """
    Evaluador 5: LLM-as-Judge para verificar tono c√≥mico
    M√©trica: ¬øEl art√≠culo mantiene un tono c√≥mico e inteligente?
    """
    
    try:
        articulo = ejemplo.inputs.get("articulo", "")
        
        # Usar LangChain LLM para evaluar tono
        response = llm.invoke([
            HumanMessage(content=
                "Eres un evaluador editorial experto. Analiza el tono c√≥mico del siguiente art√≠culo. "
                "Responde SOLO con un n√∫mero del 0 al 10 (sin decimales) donde:\n"
                "0 = nada c√≥mico, 10 = extremadamente c√≥mico e inteligente\n\n"
                f"Art√≠culo:\n{articulo[:2000]}"
            )
        ])
        
        # Extraer score del contenido
        contenido = response.content.strip()
        try:
            # Extraer solo el n√∫mero
            numero = ''.join(filter(str.isdigit, contenido.split()[0]))
            score = float(numero) / 10.0  # Convertir a 0-1
        except:
            score = 0.5
        
        return {
            "key": "tono_comico",
            "score": score,
            "comment": f"Evaluaci√≥n LLM del tono c√≥mico: {score*10:.1f}/10"
        }
    except Exception as e:
        return {"key": "tono_comico", "score": 0.5, "comment": f"Error en LLM: {str(e)[:50]}"}


# ============================================================================
# CREACI√ìN DE DATASET
# ============================================================================

def crear_dataset_articulos(articulo: str) -> str:
    """Crea o recupera un dataset en LangSmith"""
    
    print(f"üìä Buscando/creando dataset: {DATASET_NAME}")
    
    try:
        # Intentar obtener dataset existente - nueva API sin par√°metro 'name'
        datasets = list(client.list_datasets())
        dataset = None
        
        # Buscar dataset por nombre
        for ds in datasets:
            if ds.name == DATASET_NAME:
                dataset = ds
                break
        
        if dataset:
            print(f"‚úÖ Dataset existente encontrado (ID: {dataset.id})")
            return dataset.id
        
        # Crear nuevo dataset
        print(f"üÜï Creando nuevo dataset...")
        dataset = client.create_dataset(dataset_name=DATASET_NAME)
        
        # Crear ejemplos
        ejemplos = [
            {
                "articulo": articulo,
                "tema": "Inteligencia Artificial en pa√≠ses del primer mundo"
            }
        ]
        
        # Agregar ejemplos al dataset
        for ejemplo in ejemplos:
            client.create_example(
                dataset_id=dataset.id,
                inputs={"articulo": ejemplo["articulo"], "tema": ejemplo["tema"]},
                outputs={"tono": "C√≥mico", "calidad": "Alta"}
            )
        
        print(f"‚úÖ Dataset creado con {len(ejemplos)} ejemplo(s)")
        return dataset.id
    
    except Exception as e:
        print(f"‚ùå Error creando dataset: {e}")
        raise


# ============================================================================
# FUNCI√ìN PARA GUARDAR REPORTE
# ============================================================================

def generar_reporte_langsmith(resultados, archivo_original: str) -> str:
    """Genera reporte de evaluaci√≥n en Markdown"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    nombre_reporte = f"evaluacion_langsmith_{timestamp}.md"
    
    # Procesar resultados
    evaluaciones = {}
    for resultado in resultados:
        key = resultado.get("key")
        score = resultado.get("score", 0)
        comment = resultado.get("comment", "")
        evaluaciones[key] = {"score": score, "comment": comment}
    
    # Calcular promedio
    scores = [e["score"] for e in evaluaciones.values() if isinstance(e["score"], (int, float))]
    promedio = sum(scores) / len(scores) if scores else 0
    
    # Contenido del reporte
    contenido = f"""# üìä Reporte de Evaluaci√≥n LangSmith

**Fecha:** {datetime.now().strftime("%d/%m/%Y %H:%M:%S")}

**Archivo Evaluado:** {archivo_original}

**Framework:** LangSmith

---

## üéØ Resumen Ejecutivo

| M√©trica | Valor |
|---------|-------|
| **Puntuaci√≥n Promedio** | {promedio:.2f}/1.0 ({promedio*100:.1f}%) |
| **Total Evaluadores** | {len(evaluaciones)} |
| **Estado** | {"‚úÖ APROBADO" if promedio >= 0.7 else "‚ö†Ô∏è REQUIERE REVISI√ìN" if promedio >= 0.5 else "‚ùå RECHAZADO"} |

---

## üìà Resultados Detallados

"""
    
    # Agregar cada evaluador
    evaluador_nombre = {
        "longitud_minima": "1Ô∏è‚É£ Longitud M√≠nima",
        "contenido_apropiado": "2Ô∏è‚É£ Contenido Apropiado",
        "estructura_parrafos": "3Ô∏è‚É£ Estructura de P√°rrafos",
        "relevancia_tema": "4Ô∏è‚É£ Relevancia del Tema",
        "tono_comico": "5Ô∏è‚É£ Tono C√≥mico (LLM)"
    }
    
    for key, eval_data in evaluaciones.items():
        score = eval_data["score"]
        comment = eval_data["comment"]
        emoji = "üü¢" if score >= 0.8 else "üü°" if score >= 0.5 else "üî¥"
        
        contenido += f"""
### {evaluador_nombre.get(key, f"Evaluador {key}")}

**Puntuaci√≥n:** {score:.2f}/1.0 ({score*100:.1f}%) {emoji}

**An√°lisis:** {comment}

"""
    
    # Interpretaci√≥n
    contenido += f"""

---

## üîç Interpretaci√≥n

### Escala de Puntuaci√≥n:
- **0.9-1.0:** Excelente ‚úÖ
- **0.7-0.89:** Bueno ‚úÖ
- **0.5-0.69:** Aceptable ‚ö†Ô∏è
- **< 0.5:** Deficiente ‚ùå

### Recomendaciones:

"""
    
    if promedio >= 0.8:
        contenido += "‚ú® **El art√≠culo est√° listo para publicaci√≥n.** Cumple con todos los est√°ndares de calidad editorial."
    elif promedio >= 0.6:
        contenido += "‚ö†Ô∏è **El art√≠culo necesita algunos ajustes menores** antes de publicar. Revisa los evaluadores con puntuaci√≥n baja."
    else:
        contenido += "‚ùå **El art√≠culo requiere revisi√≥n significativa.** Se recomienda reescritura seg√∫n feedback de evaluadores."
    
    contenido += f"""

---

## üõ†Ô∏è Informaci√≥n T√©cnica

- **Plataforma:** LangSmith
- **Dataset:** {DATASET_NAME}
- **Experimento:** {EXPERIMENT_PREFIX}
- **Modelo Evaluador:** GPT-4 (para evaluaciones LLM)
- **Evaluadores Utilizados:** {len(evaluaciones)}

### Evaluadores Implementados:

1. **Longitud M√≠nima** - Regla determin√≠stica
2. **Contenido Apropiado** - Regla determin√≠stica
3. **Estructura de P√°rrafos** - M√©trica cuantitativa
4. **Relevancia del Tema** - An√°lisis de palabras clave
5. **Tono C√≥mico** - LLM-as-Judge

---

## üìã Pr√≥ximos Pasos

1. Revisar feedback de evaluadores
2. Si aprobado (‚â•0.7): Publicar art√≠culo
3. Si requiere revisi√≥n (0.5-0.7): Ajustar seg√∫n comentarios
4. Si rechazado (<0.5): Considerar reescritura

---

*Reporte generado autom√°ticamente por LangSmith*
"""
    
    # Guardar archivo
    try:
        with open(nombre_reporte, 'w', encoding='utf-8') as f:
            f.write(contenido)
        print(f"‚úÖ Reporte guardado: {nombre_reporte}")
        return nombre_reporte
    except Exception as e:
        print(f"‚ùå Error al guardar reporte: {e}")
        return None


# ============================================================================
# FUNCI√ìN PRINCIPAL
# ============================================================================

def main():
    """Funci√≥n principal de evaluaci√≥n con LangSmith"""
    
    print("=" * 80)
    print("üî¨ EVALUACI√ìN CON LANGSMITH - ART√çCULOS EDITORIALES")
    print("=" * 80)
    print()
    
    # Obtener archivo
    if len(sys.argv) > 1:
        archivo_markdown = sys.argv[1]
    else:
        archivos = list(Path(".").glob("articulo_editorial_*.md"))
        if archivos:
            archivo_markdown = str(archivos[-1])
            print(f"üìÅ Archivo detectado: {archivo_markdown}\n")
        else:
            print("‚ùå No se encontr√≥ archivo de art√≠culo editorial")
            print("Uso: python langsmith_evaluador.py <archivo_markdown>")
            sys.exit(1)
    
    # Extraer art√≠culo
    print(f"üìñ Extrayendo art√≠culo...")
    articulo = extraer_articulo_del_markdown(archivo_markdown)
    
    if not articulo:
        print("‚ùå No se pudo extraer el art√≠culo")
        sys.exit(1)
    
    print(f"‚úÖ Art√≠culo extra√≠do ({len(articulo)} caracteres)\n")
    
    # Crear dataset
    print("=" * 80)
    print("üìä CREANDO DATASET EN LANGSMITH")
    print("=" * 80)
    print()
    
    try:
        dataset_id = crear_dataset_articulos(articulo)
    except Exception as e:
        print(f"‚ö†Ô∏è Advertencia: No se pudo crear dataset: {e}")
        print("Continuando con evaluaci√≥n local...\n")
        dataset_id = None
    
    # Ejecutar evaluaci√≥n
    print("\n" + "=" * 80)
    print("üöÄ EJECUTANDO EVALUACI√ìN")
    print("=" * 80)
    print()
    
    try:
        # Crear objeto Example con UUID
        import uuid
        
        ejemplo = Example(
            id=str(uuid.uuid4()),
            inputs={"articulo": articulo, "tema": "Inteligencia Artificial"},
            outputs={"tono": "C√≥mico"}
        )
        
        resultados_evaluacion = []
        evaluadores = [
            evaluador_longitud_minima,
            evaluador_contenido_apropiado,
            evaluador_cantidad_parrafos,
            evaluador_relevancia_tema,
            evaluador_tono_comico_llm
        ]
        
        print("Ejecutando evaluadores:\n")
        
        for evaluador_func in evaluadores:
            try:
                # Llamar evaluador sin Run
                resultado = evaluador_func(ejemplo)
                resultados_evaluacion.append(resultado)
                print(f"‚úì {resultado['key']}: {resultado['score']:.2f}/1.0")
                print(f"  ‚Üí {resultado['comment']}\n")
            except Exception as e:
                print(f"‚ùå Error en {evaluador_func.__name__}: {e}\n")
        
        # Mostrar resumen
        print("=" * 80)
        print("üìã RESUMEN DE EVALUACI√ìN")
        print("=" * 80)
        print()
        
        promedio = sum(r["score"] for r in resultados_evaluacion) / len(resultados_evaluacion)
        print(f"Puntuaci√≥n Promedio: {promedio:.2f}/1.0 ({promedio*100:.1f}%)\n")
        
        # Generar reporte
        print("=" * 80)
        print("üíæ GENERANDO REPORTE")
        print("=" * 80)
        print()
        
        reporte_path = generar_reporte_langsmith(resultados_evaluacion, archivo_markdown)
        
        if reporte_path:
            print(f"üìÅ Ubicaci√≥n: {os.path.abspath(reporte_path)}")
            print(f"\n‚ú® ¬°Evaluaci√≥n completada exitosamente!")
    
    except Exception as e:
        print(f"‚ùå Error durante evaluaci√≥n: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()